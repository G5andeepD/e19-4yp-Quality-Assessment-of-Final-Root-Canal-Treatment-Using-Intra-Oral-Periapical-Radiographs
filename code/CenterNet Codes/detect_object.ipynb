{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting albumentations\n",
      "  Downloading albumentations-2.0.0-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting numpy>=1.24.4 (from albumentations)\n",
      "  Downloading numpy-2.2.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\ajani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (1.10.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\ajani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (6.0.1)\n",
      "Collecting pydantic>=2.9.2 (from albumentations)\n",
      "  Downloading pydantic-2.10.5-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting albucore==0.0.23 (from albumentations)\n",
      "  Downloading albucore-0.0.23-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in c:\\users\\ajani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from albumentations) (4.9.0.80)\n",
      "Collecting stringzilla>=3.10.4 (from albucore==0.0.23->albumentations)\n",
      "  Downloading stringzilla-3.11.3-cp311-cp311-win_amd64.whl.metadata (81 kB)\n",
      "Collecting simsimd>=5.9.2 (from albucore==0.0.23->albumentations)\n",
      "  Downloading simsimd-6.2.1-cp311-cp311-win_amd64.whl.metadata (67 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9.2->albumentations)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic>=2.9.2->albumentations)\n",
      "  Downloading pydantic_core-2.27.2-cp311-cp311-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic>=2.9.2->albumentations)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting numpy>=1.24.4 (from albumentations)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Downloading albumentations-2.0.0-py3-none-any.whl (273 kB)\n",
      "Downloading albucore-0.0.23-py3-none-any.whl (14 kB)\n",
      "Downloading pydantic-2.10.5-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 2.0/2.0 MB 6.5 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 15.8/15.8 MB 5.2 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading simsimd-6.2.1-cp311-cp311-win_amd64.whl (86 kB)\n",
      "Downloading stringzilla-3.11.3-cp311-cp311-win_amd64.whl (80 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: stringzilla, simsimd, typing-extensions, numpy, annotated-types, pydantic-core, pydantic, albucore, albumentations\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.2\n",
      "    Uninstalling numpy-1.24.2:\n",
      "      Successfully uninstalled numpy-1.24.2\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.17\n",
      "    Uninstalling pydantic-1.10.17:\n",
      "      Successfully uninstalled pydantic-1.10.17\n",
      "Successfully installed albucore-0.0.23 albumentations-2.0.0 annotated-types-0.7.0 numpy-1.26.4 pydantic-2.10.5 pydantic-core-2.27.2 simsimd-6.2.1 stringzilla-3.11.3 typing-extensions-4.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ajani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastapi-utils 0.2.1 requires pydantic<2.0,>=1.0, but you have pydantic 2.10.5 which is incompatible.\n",
      "mlserver 1.5.0 requires pydantic<2.0.0, but you have pydantic 2.10.5 which is incompatible.\n",
      "mlserver-mlflow 1.5.0 requires pydantic<2.0.0, but you have pydantic 2.10.5 which is incompatible.\n",
      "sqlmodel 0.0.8 requires pydantic<2.0.0,>=1.8.2, but you have pydantic 2.10.5 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.4 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.12.2 which is incompatible.\n",
      "zenml 0.58.2 requires pydantic<1.11,>=1.9.0, but you have pydantic 2.10.5 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install albumentations\n",
    "%pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\ajani\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.4)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Downloading numpy-2.2.1-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 12.9/12.9 MB 6.6 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-2.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mlflow 2.14.1 requires numpy<2, but you have numpy 2.2.1 which is incompatible.\n",
      "mlserver 1.5.0 requires pydantic<2.0.0, but you have pydantic 2.10.5 which is incompatible.\n",
      "mlserver-mlflow 1.5.0 requires pydantic<2.0.0, but you have pydantic 2.10.5 which is incompatible.\n",
      "pyarrow 15.0.2 requires numpy<2,>=1.16.6, but you have numpy 2.2.1 which is incompatible.\n",
      "scipy 1.10.1 requires numpy<1.27.0,>=1.19.5, but you have numpy 2.2.1 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 2.2.1 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.12.2 which is incompatible.\n",
      "zenml 0.58.2 requires pydantic<1.11,>=1.9.0, but you have pydantic 2.10.5 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall numpy scipy pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50\n",
    "import numpy as np\n",
    "\n",
    "class CenterNetDental(nn.Module):\n",
    "    def __init__(self, num_classes=1, output_size=(128, 128)):\n",
    "        super(CenterNetDental, self).__init__()\n",
    "        \n",
    "        # ResNet50 backbone with pretrained weights\n",
    "        backbone = resnet50(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
    "        \n",
    "        # Upsampling layers\n",
    "        self.deconv_layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2048, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Head layers for different tasks\n",
    "        self.heatmap_head = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        self.wh_head = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 2, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        self.offset_head = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 2, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Backbone feature extraction\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Upsampling\n",
    "        features = self.deconv_layers(features)\n",
    "        \n",
    "        # Get predictions\n",
    "        heatmaps = self.heatmap_head(features)\n",
    "        wh = self.wh_head(features)\n",
    "        offset = self.offset_head(features)\n",
    "        \n",
    "        # Apply sigmoid to heatmaps\n",
    "        heatmaps = torch.sigmoid(heatmaps)\n",
    "        \n",
    "        return heatmaps, wh, offset\n",
    "\n",
    "def gaussian2D(shape, sigma=1):\n",
    "    m, n = [(ss - 1.) / 2. for ss in shape]\n",
    "    y, x = np.ogrid[-m:m+1, -n:n+1]\n",
    "    \n",
    "    h = np.exp(-(x * x + y * y) / (2 * sigma * sigma))\n",
    "    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n",
    "    return h\n",
    "\n",
    "def draw_gaussian(heatmap, center, radius, k=1):\n",
    "    diameter = 2 * radius + 1\n",
    "    gaussian = gaussian2D((diameter, diameter), sigma=diameter / 6)\n",
    "    \n",
    "    x, y = int(center[0]), int(center[1])\n",
    "    \n",
    "    height, width = heatmap.shape[0:2]\n",
    "    \n",
    "    left, right = min(x, radius), min(width - x, radius + 1)\n",
    "    top, bottom = min(y, radius), min(height - y, radius + 1)\n",
    "    \n",
    "    masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]\n",
    "    masked_gaussian = gaussian[radius - top:radius + bottom, radius - left:radius + right]\n",
    "    \n",
    "    if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0:\n",
    "        np.maximum(masked_heatmap, masked_gaussian * k, out=masked_heatmap)\n",
    "    \n",
    "    return heatmap\n",
    "\n",
    "class DentalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, annotations, output_size=(128, 128), transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.annotations = annotations\n",
    "        self.output_size = output_size\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img = load_image(self.image_paths[idx])  # Implement load_image based on your needs\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # Get annotations for this image\n",
    "        boxes = self.annotations[idx]\n",
    "        \n",
    "        # Create target maps\n",
    "        heatmap = np.zeros((1, self.output_size[0], self.output_size[1]))\n",
    "        wh = np.zeros((2, self.output_size[0], self.output_size[1]))\n",
    "        offset = np.zeros((2, self.output_size[0], self.output_size[1]))\n",
    "        reg_mask = np.zeros((self.output_size[0], self.output_size[1]))\n",
    "        \n",
    "        # Process each box\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            cx = (x1 + x2) / 2\n",
    "            cy = (y1 + y2) / 2\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "            \n",
    "            # Scale to output size\n",
    "            cx = cx * self.output_size[1] / img.shape[1]\n",
    "            cy = cy * self.output_size[0] / img.shape[0]\n",
    "            w = w * self.output_size[1] / img.shape[1]\n",
    "            h = h * self.output_size[0] / img.shape[0]\n",
    "            \n",
    "            # Draw gaussian\n",
    "            draw_gaussian(heatmap[0], (cx, cy), radius=3)\n",
    "            \n",
    "            # Add width/height\n",
    "            wh[0, int(cy), int(cx)] = w\n",
    "            wh[1, int(cy), int(cx)] = h\n",
    "            \n",
    "            # Add offset\n",
    "            offset[0, int(cy), int(cx)] = cx - int(cx)\n",
    "            offset[1, int(cy), int(cx)] = cy - int(cy)\n",
    "            \n",
    "            # Add reg mask\n",
    "            reg_mask[int(cy), int(cx)] = 1\n",
    "            \n",
    "        return {\n",
    "            'image': torch.FloatTensor(img),\n",
    "            'heatmap': torch.FloatTensor(heatmap),\n",
    "            'wh': torch.FloatTensor(wh),\n",
    "            'offset': torch.FloatTensor(offset),\n",
    "            'reg_mask': torch.FloatTensor(reg_mask)\n",
    "        }\n",
    "\n",
    "def focal_loss(pred, gt):\n",
    "    pos_inds = gt.eq(1).float()\n",
    "    neg_inds = gt.lt(1).float()\n",
    "    \n",
    "    neg_weights = torch.pow(1 - gt, 4)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    pos_loss = torch.log(pred) * torch.pow(1 - pred, 2) * pos_inds\n",
    "    neg_loss = torch.log(1 - pred) * torch.pow(pred, 2) * neg_weights * neg_inds\n",
    "    \n",
    "    num_pos = pos_inds.float().sum()\n",
    "    pos_loss = pos_loss.sum()\n",
    "    neg_loss = neg_loss.sum()\n",
    "    \n",
    "    if num_pos == 0:\n",
    "        loss = loss - neg_loss\n",
    "    else:\n",
    "        loss = loss - (pos_loss + neg_loss) / num_pos\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def train_model(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        images = batch['image']\n",
    "        heatmap_target = batch['heatmap']\n",
    "        wh_target = batch['wh']\n",
    "        offset_target = batch['offset']\n",
    "        reg_mask = batch['reg_mask']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        heatmap_pred, wh_pred, offset_pred = model(images)\n",
    "        \n",
    "        # Calculate losses\n",
    "        heatmap_loss = focal_loss(heatmap_pred, heatmap_target)\n",
    "        wh_loss = F.l1_loss(wh_pred * reg_mask.unsqueeze(1), \n",
    "                           wh_target * reg_mask.unsqueeze(1),\n",
    "                           reduction='sum') / (reg_mask.sum() + 1e-4)\n",
    "        offset_loss = F.l1_loss(offset_pred * reg_mask.unsqueeze(1),\n",
    "                               offset_target * reg_mask.unsqueeze(1),\n",
    "                               reduction='sum') / (reg_mask.sum() + 1e-4)\n",
    "        \n",
    "        loss = heatmap_loss + wh_loss + offset_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from sklearn.metrics import average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DentalAugmentation:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.transform = A.Compose([\n",
    "            # Intensity adjustments to simulate different exposure levels\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.2,\n",
    "                contrast_limit=0.2,\n",
    "                p=p\n",
    "            ),\n",
    "            # Simulate different X-ray angles\n",
    "            A.ShiftScaleRotate(\n",
    "                shift_limit=0.1,\n",
    "                scale_limit=0.1,\n",
    "                rotate_limit=15,\n",
    "                border_mode=cv2.BORDER_CONSTANT,\n",
    "                p=p\n",
    "            ),\n",
    "            # Add noise to simulate different X-ray qualities\n",
    "            A.GaussNoise(\n",
    "                var_limit=(10.0, 50.0),\n",
    "                p=p\n",
    "            ),\n",
    "            # Adjust gamma to simulate different tissue densities\n",
    "            A.RandomGamma(\n",
    "                gamma_limit=(80, 120),\n",
    "                p=p\n",
    "            ),\n",
    "            # Elastic deformation to simulate tissue variation\n",
    "            A.ElasticTransform(\n",
    "                alpha=1,\n",
    "                sigma=50,\n",
    "                alpha_affine=50,\n",
    "                border_mode=cv2.BORDER_CONSTANT,\n",
    "                p=p\n",
    "            )\n",
    "        ], bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            label_fields=['labels']\n",
    "        ))\n",
    "    \n",
    "    def __call__(self, image, boxes=None):\n",
    "        if boxes is None:\n",
    "            transformed = self.transform(image=image)\n",
    "            return transformed['image']\n",
    "        \n",
    "        # Prepare boxes for augmentation\n",
    "        labels = ['tooth'] * len(boxes)\n",
    "        transformed = self.transform(\n",
    "            image=image,\n",
    "            bboxes=boxes,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        return transformed['image'], transformed['bboxes']\n",
    "\n",
    "def post_process_detections(heatmap, wh, offset, confidence_threshold=0.3, nms_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Process CenterNet outputs to get final bounding box predictions\n",
    "    \"\"\"\n",
    "    batch_size = heatmap.shape[0]\n",
    "    height = heatmap.shape[2]\n",
    "    width = heatmap.shape[3]\n",
    "    num_classes = heatmap.shape[1]\n",
    "    \n",
    "    # Get peaks in the heatmap\n",
    "    heatmap = torch.nn.functional.max_pool2d(\n",
    "        heatmap,\n",
    "        kernel_size=3,\n",
    "        padding=1,\n",
    "        stride=1\n",
    "    )\n",
    "    \n",
    "    scores, indices = torch.max(heatmap, dim=1)\n",
    "    scores = scores.view(batch_size, -1)\n",
    "    topk_scores, topk_inds = torch.topk(scores, k=100)\n",
    "    \n",
    "    topk_ys = (topk_inds / width).int().float()\n",
    "    topk_xs = (topk_inds % width).int().float()\n",
    "    \n",
    "    # Get width and height predictions\n",
    "    wh = wh.view(batch_size, 2, -1)\n",
    "    offset = offset.view(batch_size, 2, -1)\n",
    "    \n",
    "    topk_wh = torch.gather(wh, 2, \n",
    "        topk_inds.unsqueeze(1).expand(batch_size, 2, 100))\n",
    "    topk_offset = torch.gather(offset, 2,\n",
    "        topk_inds.unsqueeze(1).expand(batch_size, 2, 100))\n",
    "    \n",
    "    # Calculate bounding box coordinates\n",
    "    boxes = torch.zeros_like(topk_wh)\n",
    "    boxes[:, 0] = topk_xs + topk_offset[:, 0]  # x1\n",
    "    boxes[:, 1] = topk_ys + topk_offset[:, 1]  # y1\n",
    "    \n",
    "    # Convert center to corners\n",
    "    boxes[:, 0] -= topk_wh[:, 0] / 2  # x1\n",
    "    boxes[:, 1] -= topk_wh[:, 1] / 2  # y1\n",
    "    boxes[:, 0] += topk_wh[:, 0]      # x2\n",
    "    boxes[:, 1] += topk_wh[:, 1]      # y2\n",
    "    \n",
    "    # Filter by confidence\n",
    "    mask = topk_scores > confidence_threshold\n",
    "    boxes = boxes[mask]\n",
    "    scores = topk_scores[mask]\n",
    "    \n",
    "    # Apply NMS\n",
    "    keep = nms(boxes, scores, nms_threshold)\n",
    "    boxes = boxes[keep]\n",
    "    scores = scores[keep]\n",
    "    \n",
    "    return boxes, scores\n",
    "\n",
    "def evaluate_model(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate model performance using mean Average Precision (mAP)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            target_heatmaps = batch['heatmap']\n",
    "            \n",
    "            # Get predictions\n",
    "            heatmaps, wh, offset = model(images)\n",
    "            boxes, scores = post_process_detections(\n",
    "                heatmaps, wh, offset\n",
    "            )\n",
    "            \n",
    "            # Convert predictions and targets to common format\n",
    "            all_predictions.extend(scores.cpu().numpy())\n",
    "            all_targets.extend(target_heatmaps.view(-1).cpu().numpy())\n",
    "    \n",
    "    # Calculate mAP\n",
    "    mAP = average_precision_score(all_targets, all_predictions)\n",
    "    return mAP\n",
    "\n",
    "def visualize_predictions(image, boxes, scores, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Visualize detection results on the image\n",
    "    \"\"\"\n",
    "    image = image.copy()\n",
    "    \n",
    "    # Filter predictions by confidence\n",
    "    mask = scores > threshold\n",
    "    boxes = boxes[mask]\n",
    "    scores = scores[mask]\n",
    "    \n",
    "    # Draw boxes\n",
    "    for box, score in zip(boxes, scores):\n",
    "        x1, y1, x2, y2 = box.int()\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (x1, y1), (x2, y2),\n",
    "            color=(0, 255, 0),\n",
    "            thickness=2\n",
    "        )\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            f'{score:.2f}',\n",
    "            (x1, y1 - 5),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (0, 255, 0),\n",
    "            2\n",
    "        )\n",
    "    \n",
    "    return image\n",
    "\n",
    "def train_with_validation(model, train_loader, val_loader, \n",
    "                         optimizer, num_epochs, device):\n",
    "    \"\"\"\n",
    "    Training loop with validation\n",
    "    \"\"\"\n",
    "    best_map = 0\n",
    "    train_losses = []\n",
    "    val_maps = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_losses = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            heatmap_target = batch['heatmap'].to(device)\n",
    "            wh_target = batch['wh'].to(device)\n",
    "            offset_target = batch['offset'].to(device)\n",
    "            reg_mask = batch['reg_mask'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            heatmap_pred, wh_pred, offset_pred = model(images)\n",
    "            \n",
    "            # Calculate losses\n",
    "            loss = calculate_total_loss(\n",
    "                heatmap_pred, heatmap_target,\n",
    "                wh_pred, wh_target,\n",
    "                offset_pred, offset_target,\n",
    "                reg_mask\n",
    "            )\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_losses.append(loss.item())\n",
    "        \n",
    "        # Validation\n",
    "        mAP = evaluate_model(model, val_loader, device)\n",
    "        \n",
    "        # Save best model\n",
    "        if mAP > best_map:\n",
    "            best_map = mAP\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'mAP': mAP,\n",
    "            }, 'best_model.pth')\n",
    "        \n",
    "        # Record metrics\n",
    "        train_losses.append(np.mean(epoch_losses))\n",
    "        val_maps.append(mAP)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Training Loss: {train_losses[-1]:.4f}')\n",
    "        print(f'Validation mAP: {mAP:.4f}')\n",
    "        \n",
    "        # Plot metrics\n",
    "        plot_metrics(train_losses, val_maps)\n",
    "\n",
    "def plot_metrics(train_losses, val_maps):\n",
    "    \"\"\"\n",
    "    Plot training metrics\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    # Plot validation mAP\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_maps)\n",
    "    plt.title('Validation mAP')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('mAP')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_metrics.png')\n",
    "    plt.close()\n",
    "\n",
    "def calculate_total_loss(heatmap_pred, heatmap_target,\n",
    "                        wh_pred, wh_target,\n",
    "                        offset_pred, offset_target,\n",
    "                        reg_mask):\n",
    "    \"\"\"\n",
    "    Calculate combined loss for all predictions\n",
    "    \"\"\"\n",
    "    heatmap_loss = focal_loss(heatmap_pred, heatmap_target)\n",
    "    wh_loss = F.l1_loss(\n",
    "        wh_pred * reg_mask.unsqueeze(1),\n",
    "        wh_target * reg_mask.unsqueeze(1),\n",
    "        reduction='sum'\n",
    "    ) / (reg_mask.sum() + 1e-4)\n",
    "    offset_loss = F.l1_loss(\n",
    "        offset_pred * reg_mask.unsqueeze(1),\n",
    "        offset_target * reg_mask.unsqueeze(1),\n",
    "        reduction='sum'\n",
    "    ) / (reg_mask.sum() + 1e-4)\n",
    "    \n",
    "    return heatmap_loss + wh_loss + offset_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "from torchvision import transforms\n",
    "\n",
    "class DentalXrayDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 csv_file: str,\n",
    "                 img_dir: str,\n",
    "                 output_size: Tuple[int, int] = (128, 128),\n",
    "                 transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to CSV file with annotations\n",
    "            img_dir (str): Directory with images\n",
    "            output_size (tuple): Size of output heatmap (height, width)\n",
    "            transform: Optional transform to be applied on images\n",
    "        \"\"\"\n",
    "        # Read CSV and process data\n",
    "        self.df = pd.read_csv(csv_file, header=None, \n",
    "                             names=['filename', 'width', 'height', 'class', \n",
    "                                   'xmin', 'ymin', 'xmax', 'ymax'])\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Create class mapping\n",
    "        self.class_map = {\n",
    "            'Class 0- No endodontic treatment': 0,\n",
    "            'Class 1- complete endodontic treatment': 1,\n",
    "            'Class 2- incomplete endodontic treatment': 2,\n",
    "            'cavity': 3\n",
    "        }\n",
    "        \n",
    "        # Group annotations by filename\n",
    "        self.image_files = self.df['filename'].unique()\n",
    "        self.grouped_annotations = self.df.groupby('filename')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def _get_boxes_and_classes(self, filename: str) -> Tuple[np.ndarray, List[int]]:\n",
    "        \"\"\"Get all bounding boxes and classes for an image\"\"\"\n",
    "        annotations = self.grouped_annotations.get_group(filename)\n",
    "        boxes = annotations[['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "        classes = [self.class_map[cls] for cls in annotations['class']]\n",
    "        return boxes, classes\n",
    "    \n",
    "    def _create_heatmap(self, \n",
    "                       boxes: np.ndarray, \n",
    "                       classes: List[int], \n",
    "                       original_size: Tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"Create heatmap for CenterNet\"\"\"\n",
    "        num_classes = len(self.class_map)\n",
    "        heatmap = np.zeros((num_classes, *self.output_size))\n",
    "        \n",
    "        for box, cls_id in zip(boxes, classes):\n",
    "            # Scale box to output size\n",
    "            x1, y1, x2, y2 = box\n",
    "            x1 = x1 * self.output_size[1] / original_size[1]\n",
    "            y1 = y1 * self.output_size[0] / original_size[0]\n",
    "            x2 = x2 * self.output_size[1] / original_size[1]\n",
    "            y2 = y2 * self.output_size[0] / original_size[0]\n",
    "            \n",
    "            # Calculate center point\n",
    "            cx = (x1 + x2) / 2\n",
    "            cy = (y1 + y2) / 2\n",
    "            \n",
    "            # Create gaussian peak\n",
    "            draw_gaussian(heatmap[cls_id], (int(cx), int(cy)), radius=3)\n",
    "            \n",
    "        return heatmap\n",
    "    \n",
    "    def _create_wh_maps(self, \n",
    "                       boxes: np.ndarray, \n",
    "                       original_size: Tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"Create width-height maps\"\"\"\n",
    "        wh = np.zeros((2, *self.output_size))\n",
    "        reg_mask = np.zeros(self.output_size)\n",
    "        \n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            # Scale to output size\n",
    "            x1 = x1 * self.output_size[1] / original_size[1]\n",
    "            y1 = y1 * self.output_size[0] / original_size[0]\n",
    "            x2 = x2 * self.output_size[1] / original_size[1]\n",
    "            y2 = y2 * self.output_size[0] / original_size[0]\n",
    "            \n",
    "            # Calculate center point\n",
    "            cx = int((x1 + x2) / 2)\n",
    "            cy = int((y1 + y2) / 2)\n",
    "            \n",
    "            # Calculate width and height\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "            \n",
    "            # Add to maps\n",
    "            wh[0, cy, cx] = w\n",
    "            wh[1, cy, cx] = h\n",
    "            reg_mask[cy, cx] = 1\n",
    "            \n",
    "        return wh, reg_mask\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        # Get image filename\n",
    "        img_file = self.image_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_file)\n",
    "        \n",
    "        # Read image\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        original_size = (image.shape[0], image.shape[1])\n",
    "        \n",
    "        # Get boxes and classes\n",
    "        boxes, classes = self._get_boxes_and_classes(img_file)\n",
    "        \n",
    "        # Create target maps\n",
    "        heatmap = self._create_heatmap(boxes, classes, original_size)\n",
    "        wh, reg_mask = self._create_wh_maps(boxes, original_size)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        return {\n",
    "            'image': torch.FloatTensor(image),\n",
    "            'heatmap': torch.FloatTensor(heatmap),\n",
    "            'wh': torch.FloatTensor(wh),\n",
    "            'reg_mask': torch.FloatTensor(reg_mask),\n",
    "            'boxes': torch.FloatTensor(boxes),\n",
    "            'classes': torch.LongTensor(classes)\n",
    "        }\n",
    "\n",
    "def create_data_loaders(csv_file: str,\n",
    "                       img_dir: str,\n",
    "                       batch_size: int = 4,\n",
    "                       train_split: float = 0.8):\n",
    "    \"\"\"Create train and validation data loaders\"\"\"\n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = DentalXrayDataset(\n",
    "        csv_file=csv_file,\n",
    "        img_dir=img_dir,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Split dataset\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PE19\\FYP\\CenterNet Codes\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\PE19\\FYP\\CenterNet Codes\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\ajani/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100.0%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_with_validation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Train with validation\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43mtrain_with_validation\u001b[49m(\n\u001b[0;32m     25\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     26\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m     27\u001b[0m     val_loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[0;32m     28\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     29\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     30\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m     31\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_with_validation' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. Set up paths\n",
    "csv_file = \"train\\_annotations.csv\"\n",
    "img_dir = \"train\"\n",
    "\n",
    "# 2. Create data loaders\n",
    "train_loader, val_loader = create_data_loaders(\n",
    "    csv_file=csv_file,\n",
    "    img_dir=img_dir,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "# 3. Create and train model\n",
    "model = CenterNetDental(\n",
    "    num_classes=4,  # 3 treatment classes + cavity\n",
    "    output_size=(128, 128)\n",
    ")\n",
    "\n",
    "# 4. Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train with validation\n",
    "train_with_validation(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=50,\n",
    "    device=device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
