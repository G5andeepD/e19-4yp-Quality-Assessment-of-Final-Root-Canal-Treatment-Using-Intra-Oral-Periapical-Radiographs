{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajani\\AppData\\Local\\Temp\\ipykernel_8928\\3251259285.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_annotations['class'] = filtered_annotations['class'].map(class_mapping)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the annotations CSV file\n",
    "annotations = pd.read_csv('train/_annotations.csv')\n",
    "\n",
    "# Directory containing the filtered images\n",
    "filtered_images_dir = 'train/filtered_images'\n",
    "\n",
    "# Get the list of filenames in the filtered_images directory\n",
    "filtered_images = [f for f in os.listdir(filtered_images_dir) if os.path.isfile(os.path.join(filtered_images_dir, f))]\n",
    "\n",
    "# Filter the annotations based on filenames\n",
    "filtered_annotations = annotations[annotations['filename'].isin(filtered_images)]\n",
    "\n",
    "# Map the class labels\n",
    "class_mapping = {\n",
    "    'Class 0- No endodontic treatment': 0,\n",
    "    'Class 1- complete endodontic treatment': 1,\n",
    "    'Class 2- incomplete endodontic treatment': 1,\n",
    "    'cavity': 0,\n",
    "}\n",
    "\n",
    "# Apply the class mapping\n",
    "filtered_annotations['class'] = filtered_annotations['class'].map(class_mapping)\n",
    "\n",
    "# Remove entries where class == 0\n",
    "filtered_annotations = filtered_annotations[filtered_annotations['class'] != 0]\n",
    "\n",
    "# Save the filtered and updated annotations to a new CSV file\n",
    "filtered_annotations.to_csv('filtered_annotations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:\n",
      "Training Loss: 0.5016\n",
      "Validation Loss: 0.5415\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 192\u001b[0m\n\u001b[0;32m    189\u001b[0m     segment_and_save(model, test_loader, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegmented_output\u001b[39m\u001b[38;5;124m'\u001b[39m, DEVICE)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 185\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    182\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# Load best model and perform segmentation\u001b[39;00m\n\u001b[0;32m    188\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[1;32mIn[3], line 100\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, device, num_epochs)\u001b[0m\n\u001b[0;32m     97\u001b[0m masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     99\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 100\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, masks)\n\u001b[0;32m    102\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\PE19\\FYP\\CenterNet Codes\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PE19\\FYP\\CenterNet Codes\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 84\u001b[0m, in \u001b[0;36mUNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     82\u001b[0m d1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec1(torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample(e4), e3], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     83\u001b[0m d2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec2(torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample(d1), e2], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 84\u001b[0m d3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec3(torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43md2\u001b[49m\u001b[43m)\u001b[49m, e1], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal(d3))\n",
      "File \u001b[1;32md:\\PE19\\FYP\\CenterNet Codes\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PE19\\FYP\\CenterNet Codes\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\PE19\\FYP\\CenterNet Codes\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\upsampling.py:172\u001b[0m, in \u001b[0;36mUpsample.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrecompute_scale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecompute_scale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PE19\\FYP\\CenterNet Codes\\myenv\\Lib\\site-packages\\torch\\nn\\functional.py:4580\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   4571\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mare_deterministic_algorithms_enabled() \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m   4572\u001b[0m             \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mis_xpu\n\u001b[0;32m   4573\u001b[0m         ):\n\u001b[0;32m   4574\u001b[0m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put\u001b[39;00m\n\u001b[0;32m   4575\u001b[0m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[0;32m   4576\u001b[0m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[0;32m   4577\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\n\u001b[0;32m   4578\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch._decomp.decompositions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4579\u001b[0m             )\u001b[38;5;241m.\u001b[39m_upsample_linear_vec(\u001b[38;5;28minput\u001b[39m, output_size, align_corners, scale_factors)\n\u001b[1;32m-> 4580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample_bilinear2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4581\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\n\u001b[0;32m   4582\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   4584\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DentalDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.annotations.iloc[idx, 0])\n",
    "        image = cv2.imread(img_name, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Create mask from bounding box\n",
    "        mask = np.zeros((640, 640), dtype=np.float32)\n",
    "        if self.annotations.iloc[idx, 3] == 1:  # if treated tooth\n",
    "            xmin = self.annotations.iloc[idx, 4]\n",
    "            ymin = self.annotations.iloc[idx, 5]\n",
    "            xmax = self.annotations.iloc[idx, 6]\n",
    "            ymax = self.annotations.iloc[idx, 7]\n",
    "            mask[ymin:ymax, xmin:xmax] = 1.0\n",
    "\n",
    "        # Convert to PIL Images\n",
    "        image = Image.fromarray(image)\n",
    "        mask = Image.fromarray(mask)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = self._double_conv(1, 64)\n",
    "        self.enc2 = self._double_conv(64, 128)\n",
    "        self.enc3 = self._double_conv(128, 256)\n",
    "        self.enc4 = self._double_conv(256, 512)\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec1 = self._double_conv(512 + 256, 256)\n",
    "        self.dec2 = self._double_conv(256 + 128, 128)\n",
    "        self.dec3 = self._double_conv(128 + 64, 64)\n",
    "        self.final = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def _double_conv(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.enc4(self.pool(e3))\n",
    "        \n",
    "        # Decoder\n",
    "        d1 = self.dec1(torch.cat([self.upsample(e4), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.upsample(d1), e2], dim=1))\n",
    "        d3 = self.dec3(torch.cat([self.upsample(d2), e1], dim=1))\n",
    "        \n",
    "        return torch.sigmoid(self.final(d3))\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, masks in train_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                outputs = model(images)\n",
    "                val_loss += criterion(outputs, masks).item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Training Loss: {train_loss:.4f}')\n",
    "        print(f'Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "def segment_and_save(model, test_loader, output_dir, device):\n",
    "    model.eval()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (images, _) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Convert predictions to binary masks\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            \n",
    "            # Save segmented images\n",
    "            for i in range(predictions.shape[0]):\n",
    "                mask = predictions[i].cpu().numpy().squeeze()\n",
    "                mask = (mask * 255).astype(np.uint8)\n",
    "                cv2.imwrite(os.path.join(output_dir, f'segmented_{idx}_{i}.png'), mask)\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 4\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 50\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    dataset = DentalDataset(\n",
    "        csv_file='filtered_annotations.csv',\n",
    "        img_dir='train/filtered_images',\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Split dataset\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    val_size = int(0.15 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Initialize model, criterion, and optimizer\n",
    "    model = UNet().to(DEVICE)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Train model\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, DEVICE, NUM_EPOCHS)\n",
    "    \n",
    "    # Load best model and perform segmentation\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    segment_and_save(model, test_loader, 'segmented_output', DEVICE)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Load the filtered annotations CSV file\n",
    "annotations = pd.read_csv('filtered_annotations.csv')\n",
    "\n",
    "# Directory containing the filtered images\n",
    "filtered_images_dir = 'train/filtered_images'\n",
    "\n",
    "# Directory to save images with bounding boxes\n",
    "output_dir = 'output_images'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over each annotation\n",
    "for idx, row in annotations.iterrows():\n",
    "    img_name = row['filename']\n",
    "    img_path = os.path.join(filtered_images_dir, img_name)\n",
    "    \n",
    "    # Load the image\n",
    "    image = cv2.imread(img_path)\n",
    "    \n",
    "    # Check if the image was loaded successfully\n",
    "    if image is not None:\n",
    "        # Get bounding box coordinates\n",
    "        x_min = int(row['xmin'])\n",
    "        y_min = int(row['ymin'])\n",
    "        x_max = int(row['xmax'])\n",
    "        y_max = int(row['ymax'])\n",
    "        \n",
    "        # Draw the bounding box on the image\n",
    "        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2)\n",
    "        \n",
    "        # Save the image with bounding box\n",
    "        output_path = os.path.join(output_dir, img_name)\n",
    "        cv2.imwrite(output_path, image)\n",
    "    else:\n",
    "        print(f\"Warning: Could not load image {img_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "# Simple CNN for classification and localization\n",
    "class DentalNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DentalNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 80 * 80, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 5)  # 1 for class + 4 for bbox coordinates\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Dataset class\n",
    "class DentalDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Get class and bbox\n",
    "        label = self.annotations.iloc[idx, 3]  # class\n",
    "        bbox = self.annotations.iloc[idx, 4:8].values.astype(float)  # xmin, ymin, xmax, ymax\n",
    "\n",
    "        target = np.concatenate(([label], bbox))\n",
    "        return image, torch.FloatTensor(target)\n",
    "\n",
    "def extract_treated_teeth(model, img_dir, csv_file, output_dir,dataloader):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load data\n",
    "    dataset = DentalDataset(csv_file, img_dir)\n",
    "    # dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, targets) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Get original image name\n",
    "            img_name = dataset.annotations.iloc[i, 0]\n",
    "\n",
    "            # Get predictions\n",
    "            pred_class = outputs[0][0].item() > 0.5  # threshold at 0.5\n",
    "            pred_bbox = outputs[0][1:].cpu().numpy()\n",
    "\n",
    "            if pred_class:  # if treated tooth detected\n",
    "                # Load original image\n",
    "                orig_img = cv2.imread(os.path.join(img_dir, img_name))\n",
    "\n",
    "                # Extract coordinates\n",
    "                xmin, ymin, xmax, ymax = map(int, pred_bbox)\n",
    "\n",
    "                # Crop the tooth\n",
    "                cropped_tooth = orig_img[ymin:ymax, xmin:xmax]\n",
    "\n",
    "                # Save cropped image\n",
    "                output_path = os.path.join(output_dir, f'treated_{img_name}')\n",
    "                cv2.imwrite(output_path, cropped_tooth)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=50):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, targets in train_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Calculate loss\n",
    "            class_loss = criterion(outputs[:, 0], targets[:, 0])\n",
    "            bbox_loss = criterion(outputs[:, 1:], targets[:, 1:])\n",
    "            loss = class_loss + bbox_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# Usage\n",
    "def main():\n",
    "    # Initialize model\n",
    "    model = DentalNet()\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = DentalDataset('filtered_annotations.csv', '')\n",
    "    # Load dataset and split into train/test\n",
    "    dataset = DentalDataset('filtered_annotations.csv', '')\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Train model\n",
    "    train_model(model, train_loader, criterion, optimizer)\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'dental_model.pth')\n",
    "\n",
    "    # Extract treated teeth\n",
    "    extract_treated_teeth(\n",
    "        model,\n",
    "        '',\n",
    "        'filtered_annotations.csv',\n",
    "        'output_treated_teeth',\n",
    "        test_loader\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose and effect of each component:\n",
    "\n",
    "1. **Input Layer**:\n",
    "- The network expects grayscale images (1 channel)\n",
    "- Input size is 640x640 pixels (original image size)\n",
    "\n",
    "2. **Feature Extraction Layers (self.features)**:\n",
    "\n",
    "```python\n",
    "nn.Conv2d(1, 32, 3, padding=1)\n",
    "```\n",
    "- First Convolutional Layer\n",
    "- Input: 1 channel (grayscale)\n",
    "- Output: 32 channels\n",
    "- Kernel size: 3x3\n",
    "- Padding=1: Maintains spatial dimensions\n",
    "- Purpose: Detects basic features like edges, gradients\n",
    "- Output size: 640x640x32\n",
    "\n",
    "```python\n",
    "nn.ReLU()\n",
    "```\n",
    "- Activation function\n",
    "- Purpose: Introduces non-linearity\n",
    "- Converts negative values to zero\n",
    "- Helps network learn complex patterns\n",
    "- Reduces vanishing gradient problem\n",
    "\n",
    "```python\n",
    "nn.MaxPool2d(2)\n",
    "```\n",
    "- First Max Pooling Layer\n",
    "- Kernel size: 2x2\n",
    "- Stride: 2\n",
    "- Purpose: \n",
    "  * Reduces spatial dimensions by half\n",
    "  * Provides translation invariance\n",
    "  * Reduces computation\n",
    "- Output size: 320x320x32\n",
    "\n",
    "```python\n",
    "nn.Conv2d(32, 64, 3, padding=1)\n",
    "```\n",
    "- Second Convolutional Layer\n",
    "- Input: 32 channels\n",
    "- Output: 64 channels\n",
    "- Kernel: 3x3\n",
    "- Purpose: Detects more complex features\n",
    "- Output size: 320x320x64\n",
    "\n",
    "```python\n",
    "nn.ReLU()\n",
    "nn.MaxPool2d(2)\n",
    "```\n",
    "- Second activation and pooling\n",
    "- Further reduces dimensions\n",
    "- Output size: 160x160x64\n",
    "\n",
    "```python\n",
    "nn.Conv2d(64, 128, 3, padding=1)\n",
    "```\n",
    "- Third Convolutional Layer\n",
    "- Input: 64 channels\n",
    "- Output: 128 channels\n",
    "- Purpose: Detects high-level features\n",
    "- Output size: 160x160x128\n",
    "\n",
    "```python\n",
    "nn.ReLU()\n",
    "nn.MaxPool2d(2)\n",
    "```\n",
    "- Final activation and pooling\n",
    "- Output size: 80x80x128\n",
    "\n",
    "3. **Classifier Layers**:\n",
    "\n",
    "```python\n",
    "x = x.view(x.size(0), -1)\n",
    "```\n",
    "- Flattening operation\n",
    "- Converts 3D feature maps to 1D vector\n",
    "- Size: 128 * 80 * 80 = 819,200 features\n",
    "\n",
    "```python\n",
    "nn.Linear(128 * 80 * 80, 512)\n",
    "```\n",
    "- First Fully Connected Layer\n",
    "- Input: 819,200 features\n",
    "- Output: 512 neurons\n",
    "- Purpose: \n",
    "  * Combines all features\n",
    "  * Learns high-level representations\n",
    "  * Reduces dimensionality\n",
    "\n",
    "```python\n",
    "nn.ReLU()\n",
    "```\n",
    "- Activation for fully connected layer\n",
    "- Maintains non-linearity\n",
    "\n",
    "```python\n",
    "nn.Linear(512, 5)\n",
    "```\n",
    "- Final Output Layer\n",
    "- Input: 512 features\n",
    "- Output: 5 values\n",
    "  * First value: Classification score (treated/untreated)\n",
    "  * Next 4 values: Bounding box coordinates (xmin, ymin, xmax, ymax)\n",
    "\n",
    "4. **Architecture Design Reasoning**:\n",
    "\n",
    "A. Convolutional Layers:\n",
    "- Increasing channel depth (32 → 64 → 128)\n",
    "  * Captures increasingly complex features\n",
    "  * Earlier layers: basic edges\n",
    "  * Later layers: tooth structures, filling patterns\n",
    "\n",
    "B. Pooling Layers:\n",
    "- Reduces dimensionality gradually\n",
    "- Original: 640x640\n",
    "- After pooling: 320x320 → 160x160 → 80x80\n",
    "- Benefits:\n",
    "  * Reduces computation\n",
    "  * Provides spatial invariance\n",
    "  * Helps focus on important features\n",
    "\n",
    "C. Feature Hierarchy:\n",
    "- Layer 1: Basic edges, contrasts\n",
    "- Layer 2: Simple shapes, boundaries\n",
    "- Layer 3: Complex patterns, tooth structures\n",
    "\n",
    "D. Output Design:\n",
    "- Joint learning of classification and localization\n",
    "- Classification: Single value for treated/untreated\n",
    "- Localization: 4 values for bounding box\n",
    "- Shares feature extraction for both tasks\n",
    "\n",
    "5. **Training Considerations**:\n",
    "\n",
    "A. Parameter Count:\n",
    "- Convolutional layers: Relatively few parameters\n",
    "- Fully connected layers: Most parameters\n",
    "- Total parameters: Large due to 128*80*80 to 512 transition\n",
    "\n",
    "B. Potential Improvements:\n",
    "- Add batch normalization for better training\n",
    "- Include dropout for regularization\n",
    "- Use global average pooling to reduce parameters\n",
    "- Add skip connections for better gradient flow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
